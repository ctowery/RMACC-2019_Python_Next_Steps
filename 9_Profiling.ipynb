{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">PROFILING- Why Is It Still So Slow?<br> How Do I Run A Billion Iterations Without Taking A Long Lunch Break?</h1>\n",
    "<br>\n",
    "\n",
    "Better yet is the question, ***'Can we make it run faster?'***\n",
    "\n",
    "**Disclaimer:** Before approaching the subject of profiling and speed improvement a couple of things need to be made abundantly clear. First this is just a toy program for a functional example teaching approach and NOT something designed for production use! Second, based on the above comments this codes is NOT highly pythonic nor optimized. Third, the profiling we will be doing is old school, 'poor-man' remedial timing runs.\n",
    "\n",
    "Regardless of these facts you will see dramatic results from what we will now embark on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Runs\n",
    "\n",
    "All scaling runs use the time output from the Jupyter Notebook 'Done' message output or the QtMessagebox from our QtGuied application. Both have one simple modifictation, the addition a loop to run the 'Timing Loop' 3 times and take the average. The iterations were log increased from 100 to 1 billion iterations. The results from all scaling runs we will discuss are listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of iterations used in scale profiling tests\n",
    "numIters = np.array([100, 1000, 10000, 100000, 1000000, 10000000, 100000000, 1000000000])\n",
    "\n",
    "#functionl based script with numbe.jit enabled\n",
    "jit = np.array([0.4933, 0.5543, 0.5737, 0.5923, 0.7379, 1.3825, 8.3346, 406.154])\n",
    "\n",
    "#Final class based Qt project\n",
    "calcTime=   [0.0009, 0.0129, 0.0747, 0.6073, 6.1585, 60.7318, 601.6905, 6172.3073]\n",
    "renderTime= [0.0268, 0.0258, 0.0189, 0.0239, 0.0438,  0.4268,   1.0261,   12.5624]\n",
    "totalTime=  [0.0448, 0.0577, 0.1156, 0.6671, 6.2902, 61.2784, 605.0613, 6188.4471]\n",
    "\n",
    "staticJit=  [0.0156, 0.0313, 0.0489, 0.2356, 1.9807,  19.4169, 193.2154, 2083.1466]\n",
    "\n",
    "#Pure c++ and OpenGL using pixel binning for shading (No Datashader)\n",
    "pureC =     [0.0093, 0.0147, 0.0706,0.1776, 0.2213, 0.4147, 2.5003, 121.8462]\n",
    "\n",
    "#Speed increase seen when using numba.jit\n",
    "pdiff = totalTime/jit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function based code with numba.jit enhancement VS the class approach with no numba.jit\n",
    "\n",
    "The first thing we will look at is the comparision of our first function based code which included numba.jit versus the sad effects of turning our code into a class based approach but which was incapable of running our original numba.jit enhancement.\n",
    "\n",
    "As we know just from the 'feel' of it running with numba.jit is indeed much faster, uhm compiled code-duhhh, verses non-jit enhanced code. But now we can see just how much numba jit has helped us!\n",
    "\n",
    "Note the beatiful(?) linear nature fo the plot. Remember we are workign with a purely iterative function making it linear in its scaling. Tghe advantage is once you get a feel for the run times for given iteration variations you can fairly accurately predict the runtime for a new iteration. Meaning increase the iterations by 1 magnitude and the time will increase by 1 magnitude!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12, 4),sharey=True)\n",
    "fig.suptitle('Scaling profile for numba.jit\\nLeft Plot=Linear, Right Plot=Semilogx')\n",
    "\n",
    "ax1.plot(numIters, totalTime, label='No jit')\n",
    "ax1.plot(numIters, jit, label='jit enabled')\n",
    "ax1.set_xlabel('Number of iterations')\n",
    "ax1.set_ylabel('Runtime (seconds)')\n",
    "ax1.legend(loc=0)\n",
    "ax1.grid()\n",
    "\n",
    "ax2.semilogx(numIters, calcTime, label='Calc Time')\n",
    "ax2.semilogx(numIters, renderTime, label='Render Time')\n",
    "ax2.set_xlabel('Number of iterations')\n",
    "ax2.legend(loc=0)\n",
    "ax2.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @jit speed increase over the Qt based \n",
    "Numba claims that using @jit to precompile code you can expect upwards of a 50% speed increase. We can't tell that from the above plots but if we calculate the actual percent speed increase we can see we actually do extremely well on the speed increases except at the extremes (100 and 1 billion).\n",
    "\n",
    "Why the disparity. Well when you have a simple set of linear equations and a low number of iterations over it, its just going to be, in this case, blazingly fast no matter what. One the other end, well ahh 1 billion is just simply A LOT to expect when you take into account, unless you are running on a cluster, that there is probably a lot of junk running in the background which over time will literally suck the life out of your performance. Does that really account for the huge drop in speed up at 1 billion point? I doubt it but have not investigated further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12, 4),sharey=True)\n",
    "fig.suptitle('Speedup using numba.jit\\nLeft Plot=Linear, Right Plot=Semilogx')\n",
    "\n",
    "ax1.plot(numIters, pdiff)\n",
    "ax2.semilogx(numIters, pdiff)\n",
    "ax1.set_ylabel('Speed up (%)')\n",
    "ax1.set_xlabel('Number of iterations')\n",
    "ax1.grid()\n",
    "\n",
    "ax2.set_xlabel('Number of iterations')\n",
    "ax2.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wheres the real slow down, calculating the points or rendering them?\n",
    "\n",
    "So the real question is where is the real time sink in our code, the calculation of the attractor or rendering all the points. If, like me, you spend your life rendering things you would instantly shout out RENDERING! Rendering even today can be PAINFULLY slow, as in days for a single plate and you have 5 seconds (at roughly 30fps) to render so boom say good bye to a large set of render nodes for a week (seriously!)\n",
    "\n",
    "But, happily we are not raytracing with all the latest greatest new tools thrown into the mix!\n",
    "\n",
    "So the question still remains who is slower, calculating or rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12, 4),sharey=True)\n",
    "fig.suptitle('Calculation time vs Render Time\\nLeft Plot=Linear, Right Plot=Semilogx')\n",
    "\n",
    "ax1.plot(numIters, calcTime, label='Calc Time')\n",
    "ax1.plot(numIters, renderTime, label='Render Time')\n",
    "ax1.set_xlabel('Number of iterations')\n",
    "ax1.set_ylabel('Runtime (seconds)')\n",
    "ax1.legend(loc=0)\n",
    "ax1.grid()\n",
    "\n",
    "ax2.semilogx(numIters, calcTime, label='Calc Time')\n",
    "ax2.semilogx(numIters, renderTime, label='Render Time')\n",
    "ax2.set_xlabel('Number of iterations')\n",
    "ax2.legend(loc=0)\n",
    "ax2.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly Datashader is EXTREMELY efficent at rendering large datasets!\n",
    "\n",
    "All our time is being used on calculating the data! Plus as expected, we are still seeing a nice(?) linear relationship when increasing the number of iterations.\n",
    "\n",
    "But is Datashader really static in rendertime regardless of how much data you throw at it?\n",
    "\n",
    "The below, proves that no it is not. But what is nice, at least in this usage of Datashader its pretty linear as well. Not only that but when manipulating and render 1  BILLION datapoints it still only takes ~12 seconds! Thats extremely sexy for use who now live in a world of instant coumputing gratification expectations!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12, 4),sharey=True)\n",
    "fig.suptitle('Datashader render Time Scaling\\nLeft Plot=Linear, Right Plot=Semilogx')\n",
    "\n",
    "ax1.plot(numIters, renderTime)\n",
    "ax1.set_xlabel('Number of iterations')\n",
    "ax1.set_ylabel('Runtime (seconds)')\n",
    "ax1.grid()\n",
    "\n",
    "ax2.semilogx(numIters, renderTime)\n",
    "ax2.set_xlabel('Number of iterations')\n",
    "ax2.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well thats not shocking news but what can we do about it?\n",
    "\n",
    "So we have seen that @jit does not work in classes. We have aslo explored the potential of using @jitclass() and found it pretty much prohibitive. So what are the options?\n",
    "\n",
    "First lets go back and recognize that @jit does indeed work if its JUST a function and not embedded in a class. Yet we are beginning to see the power and absolute need for classes. If Python is truely this magical 'dreamit and it already exists' language then wheres the solution? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the @staticmethod\n",
    "\n",
    "OK, how about this, what if we could have a class that had functions in it but they were not actually part of the class? Confused yet? Well heres the solution **@staticmethod** a vanillia, albeit not overly discussed, core python feature.\n",
    "\n",
    "The magic of @staticmethod is that it creates a static object, which can live within the 'container' of class but not be associated with the class 'object' itself. If you are familiar with Java it's somewhat akin to a global function but do not confuse it with Fortran or C++'s concept of a global! As for Julia and it's ability to create a new scope at almost any time, yeah well lets just not go there right now!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The basic construct of a @staticmethod is to add this decorator before the function in your class.\n",
    "- Next, unlike all (almost) other functions in a class you DO NOT add 'self' as the first or any parameter!!!\n",
    "- Now add your function code.\n",
    "- When you want to call the code you still need a handle/instance to the class but you do not need to pre-instantiate a variable as a handle to the class. \n",
    "\n",
    "Below is a quick example of how such a function is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AttractorEquations:\n",
    "    \n",
    "    @staticmethod\n",
    "    def Clifford(x,y,z,a,b,c,d,*args):\n",
    "      Xn = np.sin(a * y) + c * np.cos(a * x)\n",
    "      Yn = np.sin(b * x) + d * np.cos(b * y)\n",
    "      Zn = 0.0\n",
    "      return Xn,Yn,Zn\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several important things you need to make note of which are important 'caveats' of @staticmethod function.\n",
    "- We now have to find a way to pass all the variables we need for the function to work. Meaning in this case we can NOT use variables in the scope of the class (again we are just using the class as a shell and have no association to the class as an object! Thus no setters or class member variable access!\n",
    "- Likewise anything we return has to literally be returned, no getters!\n",
    "- One other thing is the '\\*arg' parameter. This is cool and we have seen it earlier but now we are using the concept of \\*args slightly differently. Remember we actually have 7 different attractor variables we are making use of in all our various attractor equations. Yet for Clifford we only need 4 (a,b,c,d). Since we desire a generic method to handle any of the equations regardless of the parameters needed we pass all of them to any of your attractor equation classes. So in this case the empty variables e,f, and g are just caught with the \\*args variable and simply ignored after that point. It's only 3 variables in this case but remember the 167 parameter equation I menetioned earlier? How would you like to deal with that for EVERY equation function? That brings up the question then of how would you deal with passing 167 parameters? Answer: \\*args. Pass them all in one var and then parse them in your function or call another @staticmethod parsing function.\n",
    "\n",
    "\n",
    "As long as we understand and work in these confines then life is grand.\n",
    "\n",
    "Now to access this function you can either;<br>\n",
    "```python\n",
    "eqns = AttractorEquations()\n",
    "x[i+1],y[i+1],z[i+1] = eqns.Clifford(my args)\n",
    "```\n",
    "or just<br>\n",
    "```python\n",
    "x[i+1],y[i+1],z[i+1] = AttractorEquations.Clifford(my args)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @staticemethod has a sister - @classmethod\n",
    "\n",
    "We are not going to dig to much into it but there is a simular, albeit opposite, decorator function called **@classmethod**. There are three extremely important times when you could, and usually should, use @classmethod.\n",
    "- When creating an abstract base class, especially when you have functions that the child classes are intended to overload (also called overriding)!\n",
    "- Creating factory methods/classes.\n",
    "- When creating a polymorphic class with various optional constructors.\n",
    "\n",
    "What makes @classmethod different from @staticmethod, in context is in @staticmethod function you do not use 'self' becuase the function is not within the 'object' of the class just the container of it. \n",
    "\n",
    "In @classmethod you also do not include 'self' as a parameter. Instead you pass 'cls' in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Researcher(object):\n",
    "    def __init__(self, first_name, last_name):\n",
    "        self.first_name = first_name\n",
    "        self.last_name = last_name    \n",
    "\n",
    "    @classmethod\n",
    "    def from_string(cls, szName):\n",
    "        first_name, last_name = map(str, szName.split(' '))\n",
    "        gra = cls(first_name, last_name)\n",
    "        return gra\n",
    "\n",
    "funnestNameEver = Researcher.from_string('Ranomi Kromowidjojo')  \n",
    "\n",
    "print(funnestNameEver)\n",
    "\n",
    "#Ranomi Kromowidjojo is a Dutch freestyler swimmer who has the 2008 Olympic Gold medal in the 4x100 relay \n",
    "#and the 50m and 100m freestyle swims from the 2016 Olympics.\n",
    "#As far as I know she still holds the world record in the 50m freestyle short course. \n",
    "#But more exciting, to me anyways, is how insanely fun it is to say her name!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two major things to note here.\n",
    "- We can use the hosting class(object)'s class members and if desired class functions.\n",
    "- Note that when you print out the returning object from a @classmethod you actually get an object!\n",
    "\n",
    "Plus 3 not so widely known facts even amoung @classmethod users.\n",
    "- You can call a @staticmethod from a @classmethod\n",
    "- Likewise you can call another @classmethod from a @classmethod (I guess this coule be a factory of factories(?))\n",
    "- You do not have to name your 'cls' variable 'cls'. Heck you can call it 'Kromowidjojo' if you like but by convention please always use 'cls so others will know what is going on when reading your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to our @jit issue\n",
    "Well that was a nice segway but what does it have to do with getting @jit to work inside a class?\n",
    "I'm so glad you asked! Remember when you tried to find our why it was failing? It did not like being or using members or functions in a class (true this is a poor definition of what is really happening but this is the effective result).\n",
    "\n",
    "But now we have shown that we can create a class that has our equations in it and they are pure static(ish for c/c++ folks) functions. So what if we modify our function to this;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import jit\n",
    "\n",
    "class AttractorEquations:\n",
    "    \n",
    "    @staticmethod\n",
    "    @jit\n",
    "    def Clifford(x,y,z,a,b,c,d,*args):\n",
    "      Xn = np.sin(a * y) + c * np.cos(a * x)\n",
    "      Yn = np.sin(b * x) + d * np.cos(b * y)\n",
    "      Zn = 0.0\n",
    "      return Xn,Yn,Zn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE:** The ordering of the @staticmethod and @jit are critical. The function must be declared as a static method before numba can work with it without barfing errors on you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A full example of working version of our Attractor classes making use of @staticmethods and @jit\n",
    "\n",
    "\n",
    "Below is a modified version of the Notebook version of the code we created earlier which enables the ability to create our much more flexible class design work and still use numba.jit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datashader as ds\n",
    "from datashader import transfer_functions as tf\n",
    "from datashader import utils\n",
    "from numba import jit\n",
    "import time \n",
    "import os\n",
    "from colorcet import palette\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attractor_Config():\n",
    "    \"\"\"Utility class to open the Attractors configuration file\n",
    "       and extract the parameters for a choosen attractor type\"\"\"\n",
    "    \n",
    "    def __init__(self, fName=\"\"):\n",
    "        self._df = None\n",
    "        self._fName = fName\n",
    "        self._coords = []\n",
    "        self._params = []\n",
    "        \n",
    "        if self.fName is not None:\n",
    "            self.get_AttractorConfigFile()\n",
    "    \n",
    "    def get_AttractorConfigFile(self):\n",
    "        \"\"\"Open the Attractors configuration file\"\"\"\n",
    "        if os.path.isfile(self._fName) and os.access(self._fName, os.R_OK):\n",
    "            self._df = pd.read_csv(self._fName)\n",
    "            self._df.set_index('Attractor', inplace=True)   \n",
    "        else:\n",
    "            print(\"ERROR: The file is either missing or it's not readable\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    def getAttractor(self, fxn):\n",
    "        \"\"\"Get the configuration for the desired attractor\"\"\"\n",
    "        a = self._df.loc[fxn]\n",
    "\n",
    "        self._coords = [a.iloc[1], a.iloc[2], a.iloc[3]]\n",
    "\n",
    "        self._params = np.array([])\n",
    "        for v in range(4, np.size(a)):\n",
    "            self._params = np.append(self._params, a.iloc[v])\n",
    "\n",
    "        return self._coords, self._params\n",
    "    \n",
    "    @property\n",
    "    def coords(self):\n",
    "        \"\"\"Getter - return coords\"\"\"\n",
    "        return self._coords\n",
    "    \n",
    "    @property\n",
    "    def df(self):\n",
    "        \"\"\"Getter - return df\"\"\"\n",
    "        return self._df\n",
    "\n",
    "    @property\n",
    "    def fName(self):\n",
    "        \"\"\"Getter - return fName \"\"\"\n",
    "        return self._fName\n",
    "    \n",
    "    @property\n",
    "    def params(self):\n",
    "        \"\"\"Getter - return params\"\"\"\n",
    "        return self._params\n",
    "\n",
    "    \n",
    "    @df.setter\n",
    "    def df(self, val):\n",
    "        \"\"\"Setter - sets df\"\"\"\n",
    "        self._df = val\n",
    "        \n",
    "    @coords.setter\n",
    "    def coords(self, val):\n",
    "        \"\"\"Setter - sets coords\"\"\"\n",
    "        self._coords = val\n",
    "        \n",
    "    @fName.setter\n",
    "    def fName(self, val):\n",
    "        \"\"\"Setter - sets fName\"\"\"\n",
    "        self._fName = val\n",
    "        \n",
    "    @params.setter\n",
    "    def fName(self, val):\n",
    "        \"\"\"Setter - sets params\"\"\"\n",
    "        self._params = val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttractorEquations():\n",
    "    @staticmethod\n",
    "    @jit\n",
    "    def Pickover(x, y, z, a, b, c, d, *args):\n",
    "        Xn =  np.sin(a*y) - z*np.cos(b*x)\n",
    "        Yn =  z*np.sin(c*x) - np.cos(d*y)\n",
    "        Zn =  np.sin(x) \n",
    "\n",
    "        return Xn, Yn, Zn\n",
    "    \n",
    "    @staticmethod\n",
    "    @jit\n",
    "    def Clifford(x,y,z,a,b,c,d,*args):\n",
    "      Xn = np.sin(a * y) + c * np.cos(a * x)\n",
    "      Yn = np.sin(b * x) + d * np.cos(b * y)\n",
    "      Zn = 0.0\n",
    "      return Xn,Yn,Zn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attractors(object):\n",
    "    \"\"\" \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.n = 100000#00\n",
    "        \n",
    "        self.parse_kwargs(**kwargs)\n",
    "        self.parse_coords()\n",
    "        self.parse_avars()\n",
    "        self._cmap = 'bgy'\n",
    "\n",
    "    def parse_kwargs(self, **kwargs):\n",
    "        svd_opts = ['fxn', 'coords', 'avars']\n",
    "        \n",
    "        for key in svd_opts:\n",
    "          if key in kwargs:\n",
    "            setattr(self, key, kwargs[key])\n",
    "\n",
    "    def parse_coords(self):\n",
    "        self.x = self.coords[0]\n",
    "        self.y = self.coords[1]\n",
    "        self.z = self.coords[2]\n",
    "\n",
    "    def parse_avars(self):\n",
    "        self.a = self.avars[0]\n",
    "        self.b = self.avars[1]\n",
    "        self.c = self.avars[2]\n",
    "        self.d = self.avars[3]\n",
    "        self.e = self.avars[4]\n",
    "        self.f = self.avars[5]\n",
    "        self.g = self.avars[5]\n",
    "\n",
    "    @staticmethod\n",
    "    @jit\n",
    "    def trajectory(fn, x0, y0, z0, a=0, b=0, c=0, d=0, e=0, f=0, g=0, n=1000):\n",
    "        eqns = AttractorEquations()\n",
    "        fxn_dispatch = {'Clifford' : eqns.Clifford}\n",
    "        print(fn)\n",
    "       \n",
    "        x, y,z = np.zeros(n), np.zeros(n), np.zeros(n)\n",
    "        x[0], y[0], z[0] = x0, y0, z0\n",
    "\n",
    "        \n",
    "        for i in np.arange(n-1):\n",
    "            x[i+1], y[i+1], z[i+1] = fxn_dispatch[fn](x[i], y[i], z[i], a, b, c, d, e, f, g)\n",
    "        return pd.DataFrame(dict(x=x, y=y, z=z))\n",
    "    \n",
    "    def dsplot(self,  fn, coords, avars, n,cmap='bgy'):\n",
    "        \"\"\"Return a Datashader image by collecting `n` trajectory points for the given attractor `fn`\"\"\"\n",
    "        cmap = palette[cmap][::-1]      \n",
    "        df  = self.trajectory(fn, *coords, *avars, n=n)\n",
    "        cvs = ds.Canvas(plot_width = 400, plot_height = 400)\n",
    "        agg = cvs.points(df, 'x', 'y')\n",
    "        img = tf.shade(agg, cmap=cmap)\n",
    "        return img\n",
    "    \n",
    "    @property\n",
    "    def cmap(self):\n",
    "        \"\"\"Getter - return cmap\"\"\"\n",
    "        return self._cmap\n",
    "    \n",
    "    @cmap.setter\n",
    "    def cmap(self, val):\n",
    "        \"\"\"Setter - sets cmap\"\"\"\n",
    "        self._cmap = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == '__main__':\n",
    "fName = \"attractors.atr\"\n",
    "\n",
    "attrConfig = Attractor_Config(fName)\n",
    "coords, avars = attrConfig.getAttractor('Clifford')\n",
    "\n",
    "start = time.time()\n",
    "attr = Attractors(fxn='Clifford', coords=coords, avars=avars)\n",
    "attr.cmap = 'bgy'\n",
    "\n",
    "img = attr.dsplot(fn='Clifford',coords=coords, avars=avars, n=1000000,cmap='kbc')\n",
    "end = time.time()\n",
    "print('Time: {}'.format(end-start))\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But does it help?\n",
    "\n",
    "So now we did so fancy antsy pants things to our code and it magically (again Python is the best example of true magic in our world) just simply works when in all reality it probably shoulden't!\n",
    "BUT did all of the work? Is it really worth it to change our code to make this adjustment? Well, lets jump back to our profiling data.\n",
    "\n",
    "Below we have a plot which includes our previous results:<br>\n",
    "The orange line is our base function code using @jit<br>\n",
    "The blue line is our Qt project code.<br>\n",
    "The green line is our new @staticmethod + @jit code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(numIters, totalTime, label='No jit')\n",
    "ax.plot(numIters, jit, label='jit enabled')\n",
    "ax.plot(numIters, staticJit, label='Static jit')\n",
    "\n",
    "ax.set_title('Scaling profile for @staticmethod + numba.jit\\non Clifford attractor')\n",
    "ax.set_xlabel('Number of iterations')\n",
    "ax.set_ylabel('Runtime (seconds)')\n",
    "ax.legend(loc=0)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the speedup with our new code does not match up to our original purely functuion based code, it is vastly superiour to our Qt code as it currently stands!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we do something even better yet with Python?\n",
    "\n",
    "Using an old C++ with OpenGL, program I wrote many years ago a scaling run using the exact same attractor equation and parameters was done. The profiling plot below shows that even older C++/OpenGL code runs even faster then the combination of Pythons @staticmethod along with numba.jit. Or heck even Python's purely function based script. This should be a no brainer really but its interesting to see what difference, in this case, there is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(numIters, totalTime, label='No jit')\n",
    "ax.plot(numIters, jit, label='jit enabled')\n",
    "ax.plot(numIters, staticJit, label='Static jit')\n",
    "ax.plot(numIters, pureC, label='Pure c++')\n",
    "\n",
    "ax.set_title('Scaling profile for @staticmethod + numba.jit\\non Clifford attractor')\n",
    "ax.set_xlabel('Number of iterations')\n",
    "ax.set_ylabel('Runtime (seconds)')\n",
    "ax.legend(loc=0)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what does that mean for Python code enhancment?\n",
    "\n",
    "**Cython!!!** Cython allows you to write c extensions inside of python itself. Plus there are other ways to blend other langauages with Python.<br>\n",
    "\n",
    "Will it really help? This story is to be continued---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The moral of this story\n",
    "\n",
    "So the real question is, again, is it worth converting our code to add in @staticmethod and @jit? Well if you plan on rendering code with >1 billion points and doing it more then once then yes.<br>\n",
    "**Note:** I found, even after bumping the memory as high as I can in the Jupyter config files, that Jupyter Notebook croaks at 100 billion iterations. My C++ code easily (albeit wasting an entire weekend) can render at least 1 trillion iterations!!!\n",
    "\n",
    "***More importantly the answer is NO!***\n",
    "Wait what? Why the heck would I say that? Because Cython is a better solution? Maybe but lest assume no.\n",
    "OK then again, WHAT? \n",
    "The point is you now know that tools like @jit and making them work in the flexible all powerful world of classes exists. Why the heck would you intentionally design something like this and then plan to change it? Well unless your teaching a workshop. But other then this exception.<br>\n",
    "***Design it right from the beginning and there is no time wasted refactoring all your code!***\n",
    "\n",
    "**You now have a suite of new tools available to you - Go forth and create new magic of your own!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
